{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f85c1da2-a491-4e64-9a10-fc2af9dd33d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Spark Master at: spark://192.168.18.110:7077\n",
      "Application ID: app-20241113141202-0003\n",
      "Spark connection successful.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# Replace this with your Spark Master URL\n",
    "spark_master_url = \"spark://192.168.18.110:7077\"\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(spark_master_url) \\\n",
    "    .appName(\"Read CSV from MinIO bucket\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"GD5Lg2x7tLaGxShjozFw\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"H7J3mn7pI6oK35Xpuyrv6Lk4AsgymaSiQ2zwdAlu\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://192.168.18.115:9000\") \\\n",
    "    .getOrCreate()    \n",
    "\n",
    "# Access the SparkContext from the SparkSession\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Check if the application is connected to the Spark master\n",
    "def check_spark_connection():\n",
    "    try:\n",
    "        # Access the master URL and application ID\n",
    "        master_url = sc.master\n",
    "        app_id = sc.applicationId\n",
    "        \n",
    "        print(f\"Connected to Spark Master at: {master_url}\")\n",
    "        print(f\"Application ID: {app_id}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"Failed to connect to the Spark cluster.\")\n",
    "        print(f\"Error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run the connection check\n",
    "if check_spark_connection():\n",
    "    print(\"Spark connection successful.\")\n",
    "else:\n",
    "    print(\"Failed to connect to Spark. Exiting...\")\n",
    "    spark.stop()  # Stop the Spark session if connection fails\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7988c30f-e79e-4720-997b-83952e691ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+--------+------+----+-------+\n",
      "|product|country|      date|quantity|amount|card|Cust_ID|\n",
      "+-------+-------+----------+--------+------+----+-------+\n",
      "| prod_1|  China|12.12.2009|       1|    35|   Y| Cust_2|\n",
      "| prod_2|Germany|01.02.2011|       1|    40|   Y| Cust_1|\n",
      "| prod_3|    USA|17.03.2010|       1|    80|   Y| Cust_3|\n",
      "| prod_1|  China|28.06.2010|      10|   350|   Y| Cust_5|\n",
      "| prod_2|Germany|31.03.2010|       5|   200|   Y| Cust_4|\n",
      "| prod_3|    USA|20.08.2009|      20|  1600|NULL| Cust_3|\n",
      "| prod_1|    USA|11.10.2010|       2|    70|   Y| Cust_6|\n",
      "| prod_2|Germany|22.11.2009|      15|   600|   N| Cust_1|\n",
      "| prod_3|Germany|13.01.2010|       1|    80|NULL| Cust_4|\n",
      "| prod_1|    USA|04.07.2009|       2|    70|   Y| Cust_3|\n",
      "| prod_2|    USA|20.01.2010|       2|    80|NULL| Cust_6|\n",
      "| prod_3|Germany|14.09.2010|       2|   160|NULL| Cust_1|\n",
      "| prod_1| Brazil|17.07.2010|       5|   175|NULL| Cust_7|\n",
      "| prod_2|    USA|07.07.2010|      12|   480|NULL| Cust_3|\n",
      "| prod_4|unknown|12.12.2008|       1|     3|NULL| Cust_8|\n",
      "| prod_3|  China|02.01.2011|       8|   640|NULL| Cust_2|\n",
      "| prod_1|Germany|20.03.2011|      11|   385|   N| Cust_4|\n",
      "| prod_2|    USA|22.02.2010|       6|   240|NULL| Cust_6|\n",
      "| prod_3|  China|10.05.2009|       2|   160|NULL| Cust_2|\n",
      "| prod_1|Germany|06.03.2011|      10|   350|NULL| Cust_4|\n",
      "+-------+-------+----------+--------+------+----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Specify the S3 file path\n",
    "s3_csv_path = \"s3a://spark/allsales.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "df = spark.read.csv(s3_csv_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5bfadc8-41f0-4554-87df-bb74e508ec82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+--------+------+----+-------+\n",
      "|product|country|      date|quantity|amount|card|Cust_ID|\n",
      "+-------+-------+----------+--------+------+----+-------+\n",
      "| prod_1|  China|28.06.2010|      10|   350|   Y| Cust_5|\n",
      "| prod_2|Germany|31.03.2010|       5|   200|   Y| Cust_4|\n",
      "| prod_3|    USA|20.08.2009|      20|  1600|NULL| Cust_3|\n",
      "| prod_2|Germany|22.11.2009|      15|   600|   N| Cust_1|\n",
      "| prod_3|Germany|14.09.2010|       2|   160|NULL| Cust_1|\n",
      "| prod_1| Brazil|17.07.2010|       5|   175|NULL| Cust_7|\n",
      "| prod_2|    USA|07.07.2010|      12|   480|NULL| Cust_3|\n",
      "| prod_3|  China|02.01.2011|       8|   640|NULL| Cust_2|\n",
      "| prod_1|Germany|20.03.2011|      11|   385|   N| Cust_4|\n",
      "| prod_2|    USA|22.02.2010|       6|   240|NULL| Cust_6|\n",
      "| prod_3|  China|10.05.2009|       2|   160|NULL| Cust_2|\n",
      "| prod_1|Germany|06.03.2011|      10|   350|NULL| Cust_4|\n",
      "| prod_2|Germany|22.06.2010|       6|   240|NULL| Cust_1|\n",
      "| prod_3| Brazil|08.06.2009|      15|  1200|NULL| Cust_7|\n",
      "| prod_1|  China|28.08.2010|      10|   350|   N| Cust_2|\n",
      "| prod_2|Germany|31.08.2010|       5|   200|NULL| Cust_1|\n",
      "| prod_3|    USA|20.05.2009|      20|  1600|NULL| Cust_3|\n",
      "| prod_2|Germany|02.11.2009|      15|   600|NULL| Cust_1|\n",
      "| prod_3|    USA|03.01.2010|      20|  1600|NULL| Cust_3|\n",
      "| prod_2|Germany|15.01.2010|      25|  1000|NULL| Cust_1|\n",
      "+-------+-------+----------+--------+------+----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"sales_data\")\n",
    "result = spark.sql(\"SELECT * FROM sales_data WHERE amount > 100\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11db4bbb-ed4c-412e-9e89-f53803cd104b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
