- name: Install Apache Spark
  hosts: spark
  become: yes
  vars:
    spark_version: 3.5.1
    hadoop_version: 3
    spark_download_url: "https://dlcdn.apache.org/spark/spark-{{ spark_version }}/spark-{{ spark_version }}-bin-hadoop{{ hadoop_version }}.tgz"
    spark_install_dir: /opt/spark
  tasks:
    - name: Run system updates
      apt:
        update_cache: yes
        upgrade: dist
        cache_valid_time: 3600

    - name: Install OS packages
      apt:
        name: acl
        state: present

    - name: Install Java
      apt:
        name:
          - default-jre
          - default-jdk
        state: present
        update_cache: yes

    - name: Create spark group
      group:
        name: spark
        state: present

    - name: Create spark user
      user:
        name: spark
        group: spark
        create_home: yes
        shell: /bin/bash

    - name: Create Spark directory
      ansible.builtin.file:
        path: "{{ spark_install_dir }}/releases/artifacts"
        state: directory

    - name: Download Spark
      get_url:
        url: "{{ spark_download_url }}"
        dest: "{{ spark_install_dir }}/releases/artifacts/spark-{{ spark_version }}-bin-hadoop{{ hadoop_version }}.tgz"

    - name: Extract Spark
      unarchive:
        src: "{{ spark_install_dir }}/releases/artifacts/spark-{{ spark_version }}-bin-hadoop{{ hadoop_version }}.tgz"
        dest: "{{ spark_install_dir }}/releases"        
        remote_src: yes

    - name: Create symlink to Spark
      file:
        src: "{{ spark_install_dir }}/releases/spark-{{ spark_version }}-bin-hadoop{{ hadoop_version }}"
        dest: "{{ spark_install_dir }}/active"
        state: link

    - name: Change ownership of Spark directory
      file:
        path: "{{ spark_install_dir }}"
        owner: spark
        group: spark
        recurse: yes

- name: Configure Spark Master
  hosts: master
  become: yes
  vars:
    spark_version: 3.5.1
    hadoop_version: 3
    spark_download_url: "https://dlcdn.apache.org/spark/spark-{{ spark_version }}/spark-{{ spark_version }}-bin-hadoop{{ hadoop_version }}.tgz"
    spark_install_dir: /opt/spark
  tasks:
    - name: Configure Spark master
      lineinfile:
        path: "{{ spark_install_dir }}/active/conf/spark-env.sh"
        line: "SPARK_MASTER_HOST=$(hostname)"
        create: yes

    - name: Start Spark master
      command: "{{ spark_install_dir }}/active/sbin/start-master.sh"
      become_user: spark
    
    - name: Create Spark systemd service file
      become: yes
      copy:
        dest: /etc/systemd/system/spark.service
        content: |
          [Unit]
          Description=Apache Spark (Master)
          After=network.target          

          [Service]
          Type=forking
          User=spark
          Group=spark
          ExecStart={{ spark_install_dir }}/active/sbin/start-master.sh
          ExecStop={{ spark_install_dir }}/active/sbin/stop-master.sh
          Restart=on-failure

          [Install]
          WantedBy=multi-user.target

    - name: Reload systemd daemon
      become: yes
      command: systemctl daemon-reload

    - name: Enable Spark service
      become: yes
      systemd:
        name: spark
        enabled: yes

    - name: Start Spark service
      become: yes
      systemd:
        name: spark
        state: started

# - name: Configure Spark Slaves
#   hosts: slaves
#   become: yes
#   tasks:
#     - name: Add Spark master to slaves file
#       lineinfile:
#         path: "{{ spark_install_dir }}/spark/conf/slaves"
#         line: "master.example.com"
#         create: yes

#     - name: Start Spark slave
#       command: "{{ spark_install_dir }}/spark/sbin/start-slave.sh spark://master.example.com:7077"
#       become_user: spark
